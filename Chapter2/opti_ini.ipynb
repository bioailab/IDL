{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Google Colaboratory (if needed)\n",
    "Set the variable `colab` to `True` if you are using this notebook from Google Colaboratory, else set it to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False  # Set to true if working on Google Colaboratory\n",
    "\n",
    "if colab:\n",
    "    # Mount your Google Drive for storage\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Save to, and access files from, your Google Drive if using Colaboratory \n",
    "    ROOT_DIR = '/content/drive/MyDrive/IDL'\n",
    "    \n",
    "    # Fetch the git repository\n",
    "    !git clone https://github.com/AyushSomani001/IDL {ROOT_DIR}\n",
    "\n",
    "else:\n",
    "     ROOT_DIR = '..'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Cvs94Cf77x-"
   },
   "outputs": [],
   "source": [
    "reqs_file = f'{ROOT_DIR}/requirements.txt'\n",
    "!pip install -r {reqs_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"D:/Nirwan/Ayush sir/data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"D:/Nirwan/Ayush sir/saved_models/tutorial4\"\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# Transformations applied on each image => first make them a tensor, then normalize them with mean 0 and std 1\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.2861,), (0.3530,))\n",
    "                               ])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=False)\n",
    "val_loader = data.DataLoader(val_set, batch_size=1024, shuffle=False, drop_last=False)\n",
    "test_loader = data.DataLoader(test_set, batch_size=1024, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean\", (train_dataset.data.float() / 255.0).mean().item())\n",
    "print(\"Std\", (train_dataset.data.float() / 255.0).std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, _ = next(iter(train_loader))\n",
    "print(f\"Mean: {imgs.mean().item():5.3f}\")\n",
    "print(f\"Standard deviation: {imgs.std().item():5.3f}\")\n",
    "print(f\"Maximum: {imgs.max().item():5.3f}\")\n",
    "print(f\"Minimum: {imgs.min().item():5.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, act_fn, input_size=784, num_classes=10, hidden_sizes=[128, 256, 512, 1024, 512, 256, 128]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            act_fn - Object of the activation function that should be used as non-linearity in the network.\n",
    "            input_size - Size of the input images in pixels\n",
    "            num_classes - Number of classes we want to predict\n",
    "            hidden_sizes - A list of integers specifying the hidden layer sizes in the NN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the network based on the specified hidden sizes\n",
    "        layers = []\n",
    "        layer_sizes = [input_size] + hidden_sizes\n",
    "        for layer_index in range(1, len(layer_sizes)):\n",
    "            layers += [nn.Linear(layer_sizes[layer_index-1], layer_sizes[layer_index]),\n",
    "                       act_fn]\n",
    "        layers += [nn.Linear(layer_sizes[-1], num_classes)]\n",
    "        self.layers = nn.ModuleList(layers) # A module list registers a list of modules as submodules (e.g. for parameters)\n",
    "\n",
    "        self.config = {\"act_fn\": act_fn.__class__.__name__, \"input_size\": input_size, \"num_classes\": num_classes, \"hidden_sizes\": hidden_sizes}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "act_fn_by_name = {\n",
    "    \"tanh\": nn.Tanh,\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"identity\": Identity\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "\n",
    "def plot_dists(val_dict, color=\"C0\", xlabel=None, stat=\"count\", use_kde=True):\n",
    "    columns = len(val_dict)\n",
    "    fig, ax = plt.subplots(1, columns, figsize=(columns*3, 2.5))\n",
    "    fig_index = 0\n",
    "    for key in sorted(val_dict.keys()):\n",
    "        key_ax = ax[fig_index%columns]\n",
    "        sns.histplot(val_dict[key], ax=key_ax, color=color, bins=50, stat=stat,\n",
    "                     kde=use_kde and ((val_dict[key].max()-val_dict[key].min())>1e-8)) # Only plot kde if there is variance\n",
    "        key_ax.set_title(f\"{key} \" + (r\"(%i $\\to$ %i)\" % (val_dict[key].shape[1], val_dict[key].shape[0]) if len(val_dict[key].shape)>1 else \"\"))\n",
    "        if xlabel is not None:\n",
    "            key_ax.set_xlabel(xlabel)\n",
    "        fig_index += 1\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "    return fig\n",
    "\n",
    "##############################################################\n",
    "\n",
    "def visualize_weight_distribution(model, color=\"C0\"):\n",
    "    weights = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            continue\n",
    "        key_name = f\"Layer {name.split('.')[1]}\"\n",
    "        weights[key_name] = param.detach().view(-1).cpu().numpy()\n",
    "\n",
    "    ## Plotting\n",
    "    fig = plot_dists(weights, color=color, xlabel=\"Weight vals\")\n",
    "    fig.suptitle(\"Weight distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "def visualize_gradients(model, color=\"C0\", print_variance=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        net - Object of class BaseNetwork\n",
    "        color - Color in which we want to visualize the histogram (for easier separation of activation functions)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)\n",
    "    imgs, labels = next(iter(small_loader))\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "    # Pass one batch through the network, and calculate the gradients for the weights\n",
    "    model.zero_grad()\n",
    "    preds = model(imgs)\n",
    "    loss = F.cross_entropy(preds, labels) # Same as nn.CrossEntropyLoss, but as a function instead of module\n",
    "    loss.backward()\n",
    "    # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots\n",
    "    grads = {name: params.grad.view(-1).cpu().clone().numpy() for name, params in model.named_parameters() if \"weight\" in name}\n",
    "    model.zero_grad()\n",
    "\n",
    "    ## Plotting\n",
    "    fig = plot_dists(grads, color=color, xlabel=\"Grad magnitude\")\n",
    "    fig.suptitle(\"Gradient distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if print_variance:\n",
    "        for key in sorted(grads.keys()):\n",
    "            print(f\"{key} - Variance: {np.var(grads[key])}\")\n",
    "\n",
    "##############################################################\n",
    "\n",
    "def visualize_activations(model, color=\"C0\", print_variance=False):\n",
    "    model.eval()\n",
    "    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)\n",
    "    imgs, labels = next(iter(small_loader))\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "    # Pass one batch through the network, and calculate the gradients for the weights\n",
    "    feats = imgs.view(imgs.shape[0], -1)\n",
    "    activations = {}\n",
    "    with torch.no_grad():\n",
    "        for layer_index, layer in enumerate(model.layers):\n",
    "            feats = layer(feats)\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                activations[f\"Layer {layer_index}\"] = feats.view(-1).detach().cpu().numpy()\n",
    "\n",
    "    ## Plotting\n",
    "    fig = plot_dists(activations, color=color, stat=\"density\", xlabel=\"Activation vals\")\n",
    "    fig.suptitle(\"Activation distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if print_variance:\n",
    "        for key in sorted(activations.keys()):\n",
    "            print(f\"{key} - Variance: {np.var(activations[key])}\")\n",
    "\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseNetwork(act_fn=Identity()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def const_init(model, c=0.0):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data.fill_(c)\n",
    "\n",
    "const_init(model, c=0.005)\n",
    "visualize_gradients(model)\n",
    "visualize_activations(model, print_variance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_init(model, std=0.01):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data.normal_(std=std)\n",
    "\n",
    "var_init(model, std=0.01)\n",
    "visualize_activations(model, print_variance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_init(model, std=0.1)\n",
    "visualize_activations(model, print_variance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_var_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        else:\n",
    "            param.data.normal_(std=1.0/math.sqrt(param.shape[1]))\n",
    "\n",
    "equal_var_init(model)\n",
    "visualize_weight_distribution(model)\n",
    "visualize_activations(model, print_variance=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        else:\n",
    "            bound = math.sqrt(6)/math.sqrt(param.shape[0]+param.shape[1])\n",
    "            param.data.uniform_(-bound, bound)\n",
    "\n",
    "xavier_init(model)\n",
    "visualize_gradients(model, print_variance=True)\n",
    "visualize_activations(model, print_variance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseNetwork(act_fn=nn.Tanh()).to(device)\n",
    "xavier_init(model)\n",
    "visualize_gradients(model, print_variance=True)\n",
    "visualize_activations(model, print_variance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        elif name.startswith(\"layers.0\"): # The first layer does not have ReLU applied on its input\n",
    "            param.data.normal_(0, 1/math.sqrt(param.shape[1]))\n",
    "        else:\n",
    "            param.data.normal_(0, math.sqrt(2)/math.sqrt(param.shape[1]))\n",
    "\n",
    "model = BaseNetwork(act_fn=nn.ReLU()).to(device)\n",
    "kaiming_init(model)\n",
    "visualize_gradients(model, print_variance=True)\n",
    "visualize_activations(model, print_variance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_config_file(model_path, model_name):\n",
    "    return os.path.join(model_path, model_name + \".config\")\n",
    "\n",
    "def _get_model_file(model_path, model_name):\n",
    "    return os.path.join(model_path, model_name + \".tar\")\n",
    "\n",
    "def _get_result_file(model_path, model_name):\n",
    "    return os.path.join(model_path, model_name + \"_results.json\")\n",
    "\n",
    "def load_model(model_path, model_name, net=None):\n",
    "    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n",
    "    assert os.path.isfile(config_file), f\"Could not find the config file \\\"{config_file}\\\". Are you sure this is the correct path and you have your model config stored here?\"\n",
    "    assert os.path.isfile(model_file), f\"Could not find the model file \\\"{model_file}\\\". Are you sure this is the correct path and you have your model stored here?\"\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config_dict = json.load(f)\n",
    "    if net is None:\n",
    "        act_fn_name = config_dict[\"act_fn\"].pop(\"name\").lower()\n",
    "        assert act_fn_name in act_fn_by_name, f\"Unknown activation function \\\"{act_fn_name}\\\". Please add it to the \\\"act_fn_by_name\\\" dict.\"\n",
    "        act_fn = act_fn_by_name[act_fn_name]()\n",
    "        net = BaseNetwork(act_fn=act_fn, **config_dict)\n",
    "    net.load_state_dict(torch.load(model_file))\n",
    "    return net\n",
    "\n",
    "def save_model(model, model_path, model_name):\n",
    "    config_dict = model.config\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config_dict, f)\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "\n",
    "def train_model(net, model_name, optim_func, max_epochs=50, batch_size=256, overwrite=False):\n",
    "    \"\"\"\n",
    "    Train a model on the training set of FashionMNIST\n",
    "\n",
    "    Inputs:\n",
    "        net - Object of BaseNetwork\n",
    "        model_name - (str) Name of the model, used for creating the checkpoint names\n",
    "        max_epochs - Number of epochs we want to (maximally) train for\n",
    "        patience - If the performance on the validation set has not improved for #patience epochs, we stop training early\n",
    "        batch_size - Size of batches used in training\n",
    "        overwrite - Determines how to handle the case when there already exists a checkpoint. If True, it will be overwritten. Otherwise, we skip training.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(_get_model_file(CHECKPOINT_PATH, model_name))\n",
    "    if file_exists and not overwrite:\n",
    "        print(f\"Model file of \\\"{model_name}\\\" already exists. Skipping training...\")\n",
    "        with open(_get_result_file(CHECKPOINT_PATH, model_name), \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        if file_exists:\n",
    "            print(\"Model file exists, but will be overwritten...\")\n",
    "\n",
    "        # Defining optimizer, loss and data loader\n",
    "        optimizer =  optim_func(net.parameters())\n",
    "        loss_module = nn.CrossEntropyLoss()\n",
    "        train_loader_local = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "\n",
    "        results = None\n",
    "        val_scores = []\n",
    "        train_losses, train_scores = [], []\n",
    "        best_val_epoch = -1\n",
    "        for epoch in range(max_epochs):\n",
    "            ############\n",
    "            # Training #\n",
    "            ############\n",
    "            net.train()\n",
    "            true_preds, count = 0., 0\n",
    "            t = tqdm(train_loader_local, leave=False)\n",
    "            for imgs, labels in t:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = net(imgs)\n",
    "                loss = loss_module(preds, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Record statistics during training\n",
    "                true_preds += (preds.argmax(dim=-1) == labels).sum().item()\n",
    "                count += labels.shape[0]\n",
    "                t.set_description(f\"Epoch {epoch+1}: loss={loss.item():4.2f}\")\n",
    "                train_losses.append(loss.item())\n",
    "            train_acc = true_preds / count\n",
    "            train_scores.append(train_acc)\n",
    "\n",
    "            ##############\n",
    "            # Validation #\n",
    "            ##############\n",
    "            val_acc = test_model(net, val_loader)\n",
    "            val_scores.append(val_acc)\n",
    "            print(f\"[Epoch {epoch+1:2d}] Training accuracy: {train_acc*100.0:05.2f}%, Validation accuracy: {val_acc*100.0:05.2f}%\")\n",
    "\n",
    "            if len(val_scores) == 1 or val_acc > val_scores[best_val_epoch]:\n",
    "                print(\"\\t   (New best performance, saving model...)\")\n",
    "                save_model(net, CHECKPOINT_PATH, model_name)\n",
    "                best_val_epoch = epoch\n",
    "\n",
    "    if results is None:\n",
    "        load_model(CHECKPOINT_PATH, model_name, net=net)\n",
    "        test_acc = test_model(net, test_loader)\n",
    "        results = {\"test_acc\": test_acc, \"val_scores\": val_scores, \"train_losses\": train_losses, \"train_scores\": train_scores}\n",
    "        with open(_get_result_file(CHECKPOINT_PATH, model_name), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "    # Plot a curve of the validation accuracy\n",
    "    sns.set()\n",
    "    plt.plot([i for i in range(1,len(results[\"train_scores\"])+1)], results[\"train_scores\"], label=\"Train\")\n",
    "    plt.plot([i for i in range(1,len(results[\"val_scores\"])+1)], results[\"val_scores\"], label=\"Val\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Validation accuracy\")\n",
    "    plt.ylim(min(results[\"val_scores\"]), max(results[\"train_scores\"])*1.01)\n",
    "    plt.title(f\"Validation performance of {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print((f\" Test accuracy: {results['test_acc']*100.0:4.2f}% \").center(50, \"=\")+\"\\n\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def test_model(net, data_loader):\n",
    "    \"\"\"\n",
    "    Test a model on a specified dataset.\n",
    "\n",
    "    Inputs:\n",
    "        net - Trained model of type BaseNetwork\n",
    "        data_loader - DataLoader object of the dataset to test on (validation or test)\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    true_preds, count = 0., 0\n",
    "    for imgs, labels in data_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = net(imgs).argmax(dim=-1)\n",
    "            true_preds += (preds == labels).sum().item()\n",
    "            count += labels.shape[0]\n",
    "    test_acc = true_preds / count\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerTemplate:\n",
    "\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        ## Set gradients of all parameters to zero\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_() # For second-order optimizers important\n",
    "                p.grad.zero_()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        ## Apply update step to all parameters\n",
    "        for p in self.params:\n",
    "            if p.grad is None: # We skip parameters without any gradients\n",
    "                continue\n",
    "            self.update_param(p)\n",
    "\n",
    "    def update_param(self, p):\n",
    "        # To be implemented in optimizer-specific classes\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(OptimizerTemplate):\n",
    "\n",
    "    def __init__(self, params, lr):\n",
    "        super().__init__(params, lr)\n",
    "\n",
    "    def update_param(self, p):\n",
    "        p_update = -self.lr * p.grad\n",
    "        p.add_(p_update) # In-place update => saves memory and does not create computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(OptimizerTemplate):\n",
    "\n",
    "    def __init__(self, params, lr, momentum=0.0):\n",
    "        super().__init__(params, lr)\n",
    "        self.momentum = momentum # Corresponds to beta_1 in the equation above\n",
    "        self.param_momentum = {p: torch.zeros_like(p.data) for p in self.params} # Dict to store m_t\n",
    "\n",
    "    def update_param(self, p):\n",
    "        self.param_momentum[p] = (1 - self.momentum) * p.grad + self.momentum * self.param_momentum[p]\n",
    "        p_update = -self.lr * self.param_momentum[p]\n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(OptimizerTemplate):\n",
    "\n",
    "    def __init__(self, params, lr, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.param_step = {p: 0 for p in self.params} # Remembers \"t\" for each parameter for bias correction\n",
    "        self.param_momentum = {p: torch.zeros_like(p.data) for p in self.params}\n",
    "        self.param_2nd_momentum = {p: torch.zeros_like(p.data) for p in self.params}\n",
    "\n",
    "    def update_param(self, p):\n",
    "        self.param_step[p] += 1\n",
    "\n",
    "        self.param_momentum[p] = (1 - self.beta1) * p.grad + self.beta1 * self.param_momentum[p]\n",
    "        self.param_2nd_momentum[p] = (1 - self.beta2) * (p.grad)**2 + self.beta2 * self.param_2nd_momentum[p]\n",
    "\n",
    "        bias_correction_1 = 1 - self.beta1 ** self.param_step[p]\n",
    "        bias_correction_2 = 1 - self.beta2 ** self.param_step[p]\n",
    "\n",
    "        p_2nd_mom = self.param_2nd_momentum[p] / bias_correction_2\n",
    "        p_mom = self.param_momentum[p] / bias_correction_1\n",
    "        p_lr = self.lr / (torch.sqrt(p_2nd_mom) + self.eps)\n",
    "        p_update = -p_lr * p_mom\n",
    "\n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = BaseNetwork(act_fn=nn.ReLU(), hidden_sizes=[512,256,256,128])\n",
    "kaiming_init(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_model = copy.deepcopy(base_model).to(device)\n",
    "SGD_results = train_model(SGD_model, \"FashionMNIST_SGD\",\n",
    "                          lambda params: SGD(params, lr=1e-1),\n",
    "                          max_epochs=40, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDMom_model = copy.deepcopy(base_model).to(device)\n",
    "SGDMom_results = train_model(SGDMom_model, \"FashionMNIST_SGDMom\",\n",
    "                             lambda params: SGDMomentum(params, lr=1e-1, momentum=0.9),\n",
    "                             max_epochs=40, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam_model = copy.deepcopy(base_model).to(device)\n",
    "Adam_results = train_model(Adam_model, \"FashionMNIST_Adam\",\n",
    "                           lambda params: Adam(params, lr=1e-3),\n",
    "                           max_epochs=40, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathological_curve_loss(w1, w2):\n",
    "    # Example of a pathological curvature. There are many more possible, feel free to experiment here!\n",
    "    x1_loss = torch.tanh(w1)**2 + 0.01 * torch.abs(w1)\n",
    "    x2_loss = torch.sigmoid(w2)\n",
    "    return x1_loss + x2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(curve_fn, x_range=(-5,5), y_range=(-5,5), plot_3d=False, cmap=cm.viridis, title=\"Pathological curvature\"):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d') if plot_3d else fig.gca()\n",
    "\n",
    "    x = torch.arange(x_range[0], x_range[1], (x_range[1]-x_range[0])/100.)\n",
    "    y = torch.arange(y_range[0], y_range[1], (y_range[1]-y_range[0])/100.)\n",
    "    x, y = torch.meshgrid([x,y])\n",
    "    z = curve_fn(x, y)\n",
    "    x, y, z = x.numpy(), y.numpy(), z.numpy()\n",
    "\n",
    "    if plot_3d:\n",
    "        ax.plot_surface(x, y, z, cmap=cmap, linewidth=1, color=\"#000\", antialiased=False)\n",
    "        ax.set_zlabel(\"loss\")\n",
    "    else:\n",
    "        ax.imshow(z.T[::-1], cmap=cmap, extent=(x_range[0], x_range[1], y_range[0], y_range[1]))\n",
    "    plt.title(title)\n",
    "    ax.set_xlabel(r\"$w_1$\")\n",
    "    ax.set_ylabel(r\"$w_2$\")\n",
    "    plt.tight_layout()\n",
    "    return ax\n",
    "\n",
    "sns.reset_orig()\n",
    "_ = plot_curve(pathological_curve_loss, plot_3d=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_curve(optimizer_func, curve_func=pathological_curve_loss, num_updates=100, init=[5,5]):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        optimizer_func - Constructor of the optimizer to use. Should only take a parameter list\n",
    "        curve_func - Loss function (e.g. pathological curvature)\n",
    "        num_updates - Number of updates/steps to take when optimizing\n",
    "        init - Initial values of parameters. Must be a list/tuple with two elements representing w_1 and w_2\n",
    "    Outputs:\n",
    "        Numpy array of shape [num_updates, 3] with [t,:2] being the parameter values at step t, and [t,2] the loss at t.\n",
    "    \"\"\"\n",
    "    weights = nn.Parameter(torch.FloatTensor(init), requires_grad=True)\n",
    "    optimizer = optimizer_func([weights])\n",
    "\n",
    "    list_points = []\n",
    "    for _ in range(num_updates):\n",
    "        loss = curve_func(weights[0], weights[1])\n",
    "        list_points.append(torch.cat([weights.data.detach(), loss.unsqueeze(dim=0).detach()], dim=0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    points = torch.stack(list_points, dim=0).numpy()\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_points = train_curve(lambda params: SGD(params, lr=10))\n",
    "SGDMom_points = train_curve(lambda params: SGDMomentum(params, lr=10, momentum=0.9))\n",
    "Adam_points = train_curve(lambda params: Adam(params, lr=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = np.concatenate([SGD_points, SGDMom_points, Adam_points], axis=0)\n",
    "ax = plot_curve(pathological_curve_loss,\n",
    "                x_range=(-np.absolute(all_points[:,0]).max(), np.absolute(all_points[:,0]).max()),\n",
    "                y_range=(all_points[:,1].min(), all_points[:,1].max()),\n",
    "                plot_3d=False)\n",
    "ax.plot(SGD_points[:,0], SGD_points[:,1], color=\"magenta\", marker=\"+\", zorder=1, label=\"SGD\")\n",
    "ax.plot(SGDMom_points[:,0], SGDMom_points[:,1], color=\"yellow\", marker=\"o\", zorder=2, label=\"SGDMom\")\n",
    "ax.plot(Adam_points[:,0], Adam_points[:,1], color=\"blue\", marker=\"x\", zorder=3, label=\"Adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivar_gaussian(w1, w2, x_mean=0.0, y_mean=0.0, x_sig=1.0, y_sig=1.0):\n",
    "    norm = 1 / (2 * np.pi * x_sig * y_sig)\n",
    "    x_exp = (-1 * (w1 - x_mean)**2) / (2 * x_sig**2)\n",
    "    y_exp = (-1 * (w2 - y_mean)**2) / (2 * y_sig**2)\n",
    "    return norm * torch.exp(x_exp + y_exp)\n",
    "\n",
    "def comb_func(w1, w2):\n",
    "    z = -bivar_gaussian(w1, w2, x_mean=1.0, y_mean=-0.5, x_sig=0.2, y_sig=0.2)\n",
    "    z -= bivar_gaussian(w1, w2, x_mean=-1.0, y_mean=0.5, x_sig=0.2, y_sig=0.2)\n",
    "    z -= bivar_gaussian(w1, w2, x_mean=-0.5, y_mean=-0.8, x_sig=0.2, y_sig=0.2)\n",
    "    return z\n",
    "\n",
    "_ = plot_curve(comb_func, x_range=(-2,2), y_range=(-2,2), plot_3d=True, title=\"Steep optima\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_points = train_curve(lambda params: SGD(params, lr=.5), comb_func, init=[0,0])\n",
    "SGDMom_points = train_curve(lambda params: SGDMomentum(params, lr=1, momentum=0.9), comb_func, init=[0,0])\n",
    "Adam_points = train_curve(lambda params: Adam(params, lr=0.2), comb_func, init=[0,0])\n",
    "\n",
    "all_points = np.concatenate([SGD_points, SGDMom_points, Adam_points], axis=0)\n",
    "ax = plot_curve(comb_func,\n",
    "                x_range=(-2, 2),\n",
    "                y_range=(-2, 2),\n",
    "                plot_3d=False,\n",
    "                title=\"Steep optima\")\n",
    "ax.plot(SGD_points[:,0], SGD_points[:,1], color=\"red\", marker=\"o\", zorder=3, label=\"SGD\", alpha=0.7)\n",
    "ax.plot(SGDMom_points[:,0], SGDMom_points[:,1], color=\"blue\", marker=\"o\", zorder=2, label=\"SGDMom\", alpha=0.7)\n",
    "ax.plot(Adam_points[:,0], Adam_points[:,1], color=\"grey\", marker=\"o\", zorder=1, label=\"Adam\", alpha=0.7)\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad1c0dbef3f641073a34f09f455f49b782999c8c2b68a6221dd4f2e8b792020f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
